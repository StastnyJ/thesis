\chapter{Introduction}\label{chapter:introduction}

Nowadays, mobile robots are becoming more and more popular. Whether it is an autonomous car, a transport robot in a warehouse, or an automatic vacuum cleaner, these robots are finding more and more applications and becoming a part of most people's everyday lives.\par
One of the essential tasks every mobile robot must be able to solve is finding a valid path from its current position to the target based on the obstacles and surroundings. Knowledge of the robot's precise location and environment map is a necessary prerequisite for almost any navigation and planning algorithm.\par
Even if most of the world is already mapped on a large scale and due to GPS, Gallileo, GNSS, etc., we can locate ourselves on these maps with satisfiable accuracy, these methods can not be usually used for the indoors or small scale environments, where the maps are typically unknown, and any global positioning services can not be used.\par
Furthermore, a proper self-localization of the robot depends on a precise map, and map construction depends on the accurate positions of a robot and other landmarks. Therefore the localization and mapping problems have to be usually solved simultaneously. In addition, the environment can frequently change, which makes, together with the mutual dependence of localization and mapping, the simultaneous localization and mapping (SLAM) a very challenging problem, that most mobile robots must solve.\par
The SLAM has been solved by many scientists since the 1980s. Until now, there have been developed many different techniques for solving the SLAM, with various advantages and disadvantages. The majority of the approaches can be separated into three main categories: conventional SLAM, visual SLAM, and biologically inspired SLAM.\par
The conventional algorithms are based on a probabilistic model and usually work with Light Detection And Ranging \cite{LiDar} and odometry sensors. These techniques typically work in two steps. In the first step, the position of the robot and landmarks are extracted from the raw sensor data, usually using different filtering techniques, such as Kalman filter or particle filtering. This extracted information is used in the second step to build or update the final map.\cite{convSLAM}\par
The precision and resolution of the final maps built by these approaches are usually very high. However, there is usually a high computation and storage demand that rapidly increases with the number of landmarks. Because of this fact, the conventional techniques are generally not suitable for larger or complicated environments with a lot of landmarks and can not be performed on low-performance computation devices, such as older Raspberry PI models. Furthermore, many of these techniques usually rely on accurate sensor measurements and are not robust against more significant measurement errors that can easily destroy the whole map.\par
The visual SLAM approaches became popular mostly during the last decade, with cameras' significant cost reduction and quality improvement. As the name suggests, these techniques are based on visual input from a 2D or 3D camera and various computer vision techniques. Compared to conventional methods, these approaches obtain more information about the environment and, therefore, can generate more precise outputs. However, most visual SLAM methods are susceptible to ambient lighting and reflections and perform differently in different light conditions, which can cause significant errors. Furthermore, these techniques work poorly in a low-texture environment, making them unsuitable for environments with many windows, mirrors, or other glass or reflective surfaces.\cite{visualSLAM}\par
As the name suggests, the biologically inspired SLAM approaches find their inspiration in various biological systems. The ideas behind these techniques are very diverse and differ from approach to approach. In this category, we include techniques based on machine learning, models of biological structures, or methods based on the behavior of some biological species. These techniques usually can not guarantee the result's precision but use a heuristic approach to approximate the results with specified accuracies. These techniques generally have a significantly lower demand for resources than conventional and visual approaches. Furthermore, these techniques are usually more robust against measurement errors and can also contain a mechanism to repair the previous errors based on the new data.\par
One of the most generally known biologically inspired SLAM systems is RarSALM, which was initially developed in 2008 and improved over the years \cite{RatSLAM}. The open source version of this technique, the OpenRatSLAM \cite{OpenRatSLAM}, and its ROS implementation RatSLAMRos \cite{RatSLAMROS} brings a standardized, reconfigurable, and modulized way to include this method to any program for mobile robots. Furthermore, the RatSLAM approach shows long-term stability in the indoors and outdoors scenarios.\cite{RatSLAMExp1}\cite{RatSLAMExp2}\cite{RatSLAMExp3}\cite{RatSLAMExp4}\par
This approach is inspired by computational models of the hippocampus of rodents, which have been extensively studied concerning navigation tasks and show many of the properties of a desirable SLAM solution. During the last 50 years, four essential kinds of neurons have been discovered connected with SLAM and navigation tasks: place cells, grid cells, head direction cells, and border cells.\par
Place cells, discovered by John O'Keefe in 1976 \cite{placeCells}, are connected with different places the rodent has visited and are activated every time the rat returns to a particular location. Grid cells, discovered by Edvard and May-Britt Moser in 2008 \cite{gridCells}, react to the rodent's movement and are activated in sequence as the rat moves around in the environment. The head direction cells allow the rodent to get the spatial sense of direction based on geometry features. Finally, the border cells are activated when the rodent moves close to a wall or other obstacle.\par
According to biological inspiration, localization is based on odometry, inspired by the Grid cells. Odometry is well known for its problems with cumulative errors. It does not matter how precise odometry sensors or techniques are used; even a very small error adds up over time, and the whole system will be completely out of reality after a few minutes, maximally a few hours. To reduce these errors, a loop closure technique is required. Based on the biological model, the RatSLAM implements the loop closure using a place recognition technique.\par
So, place recognition is one of the crucial parts of the RatSLAM solution and any intelligent system operating autonomously over a longer period of time. The main task of this problem is to tell if the robot has visited the current place before or not, despite severe changes in its appearance due to different light conditions, weather, or non-stationary objects like pedestrians or cars.\par
Most standard place recognition techniques are based on visual input and usually use machine learning, feature extraction and matching, or the scene decomposition approaches. However, some other methods exist based on entirely different ideas and kinds of sensors.


\section{Motivation}\label{section:motivation}

The original RatSLAM approach uses a low-resolution camera image as an input for place recognition. However, as mentioned before, this brings some drawbacks, like sensitivity to the different light conditions and reflections. Compared to a camera, a 3D LiDAR sensor can measure directly in three dimensions with a high precision, even over a long distance. Furthermore, the 3D-LiDAR sensor is robust against different light conditions and reflections. On the other hand, compared to the camera, the LiDAR data lose some helpful information, like colors, that can be a critical factor in place recognition of places in the environments like office buildings with different meeting rooms that differ only in the wall color and otherwise remain identical.\par
The proper combination of the advantages of both these sensors can significantly improve place recognition and consequently enhance the quality of the entire RatSLAM algorithm. Furthermore, the additional odometry sensor may provide more accurate speed data, improving the SLAM quality compared to the original RatSLAM, which calculates odometry information only from the visual input.\par
Lastly, place recognition based on visual data requires storing whole images or extracted feature vectors, which consumes a relatively large amount of memory. The proper representation of the scene based on the LiDAR and camera data may significantly improve the required space and make RatSLAM even more suitable for the low performant computational devices.

\section{Objectives}\label{section:objectives}

This thesis aims to find an optimal method of combining data from several sensors and find the best solution for the place recognition problem and, as a result, improve the precision and performance of the RatSLAM algorithm. Besides, all suggested approaches find inspiration in biological systems, like the rest of the RatSLAM approach.\par
To achieve this primary goal, several challenges need to be solved. The first challenge is data fusion. In this part, the optimal scene representation must be designed to combine most of the information from all the sensors and reduce most of the disturbing factors typical for the input sensors. Furthermore, the synchronization and mutual calibration of the sensor must be solved.\par
The other challenge is to solve the place recognition problem based on the representation from the fused data. In this part, we need to think apart from accuracy to the performance and memory consumption in order to make this approach available for low-performance devices.\par
There will be suggested several algorithms with different expected advantages and disadvantages. All proposed techniques will be tested on accuracy and various performance metrics and compared to each other, as well as to an image-based place recognition used in an original RatSLAM.

\section{Work contribution}\label{section:workContribution}

First, I came up with a compact scene representation that combines the advantages of both LiDAR and the camera. Then I suggested a practical approach to fuse the data received from the sensors into the chosen representation. This part also contained sensor synchronization and calibration.\par
Afterward, I designed a compact and suitable scene template that will be stored in the memory. While creating the template, I followed the biological inspiration, namely the model of human spatial memory. I also devised a method to extract the designed template from the fused scene representation effectively. Finally, after preparing the template model, I created an approach to compare two of these templates and thus solved the scene recognition problem.\par
The suggested algorithm was based on scene decomposition, object detection, and extraction of the significant properties of each object. However, this approach did not consider the objects' exact shape, leading to a considerable increase of false positive evaluations in several environments. To eliminate these false positive evaluations, I came up with a 2 stage approach. This approach was based on the originally designed method. All scenes were evaluated before; however, all positive evaluations were re-evaluated using a neural network in the second stage.\par
To create a working second stage, I choose suitable network architecture and experimentally found the best network's dimensions. After deciding on the network architecture, I made appropriate training datasets and trained and tested the model.\par
The suggested approach was dependent on many different parameters. The optimal parameters had to be found to maximize the place recognition performance. So, I came up with an optimization method that finds an optimal parameters vector, maximizing the approach's accuracy. In order to implement this optimization method, I created a set of tools for manual and automatic performance testing and analysis.\par
After the scene recognition algorithm was finished, I successfully integrated it with RatSLAM and created a robust biologically inspired SLAM system. I made the whole system in ROS to ensure its modularity, reconfigurability, reusability, and easy improvement.\par
After creating the whole system, I devised several evaluation metrics to test all critical methods' properties. I set up a robust testing simulation in several environments and created reusable testing datasets to ensure the reproducibility of all experiments. Finally, I tested all suggested techniques and the OpenRatSLAM approach, found an appropriate results presentation, and compared all presented methods.

\section{Outline}\label{section:outline}

\textbf{Chapter 2} presents a related work about various scene recognition approaches and biologically inspired SLAM systems.\\
\textbf{Chapter 3} describes all used frameworks and tools used in this work.\\
\textbf{Chapter 4} provides a detailed description of all the algorithms and techniques implemented in this thesis. Namely, the complete system overview is provided in the beginning, followed by the solution to the data fusion problem. Afterward, three different place recognition techniques are offered.\\
\textbf{Chapter 5} evaluates the performance of approaches presented in chapter 4 and compares them with the OpenRatSLAM. The beginning of the chapter introduces the test environments and evaluation metrics. The following four sections will discuss the results of the different evaluation metrics. In the penultimate section will be tested the integration of the place recognition approaches with RatSLAM. Finally, the last section summarizes all measured results and discusses all suggested techniques' final performance and advantages or disadvantages.\\
\textbf{Chapter 6} summarizes the whole work, alerts to possible drawbacks, and suggests future work.\\
