\chapter{Conclusion and Future Work}\label{chapter:conclusion}

In this thesis, the suitable scene representation and fusion of the data from LiDAR and camera sensors were performed. Afterward, two different approaches for the solution to scene recognition were suggested, and the whole system was integrated with the RatSLAM algorithm. Both proposed techniques operated smoothly during several experiments and significantly outperformed visual scene recognition and OpenRatSLAM. \par
The addition of a 3D LiDAR sensor to the camera proved beneficial. Not only were the results generated by the proposed methods more precise than those generated by OpenRatSLAM, but they also proved to work in environments where the OpenRatSLAM fails.

\section{Limitations}

Even if the approaches proved good performance in several experiments, there is still a possibility of unknown problems that may occur under several conditions.

\subsection*{Environments}

In this thesis, all simulations were performed in static environments. However, in praxis, the environments are rather dynamic and change over time, which might cause problems with scene recognition. Dynamic environments can also contain moving objects, which 3D LiDAR might poorly detect.\par
Furthermore, the approaches were optimized for indoor environments. In outdoor environments, the ground removal technique might fail because of rugged terrain. Outdoor scenes might also be empty or contain only very far objects, so the DBScan might detect zero clusters which can cause many false negative evaluations.

\subsection*{Sensors}

The addition of a LiDAR sensor to the system significantly lowers the sensors' frequency compared to the camera-only configuration. Even if this proved to not be a problem in section \ref{section:RatSalmIntegration}, it could still cause inaccuracies for mobile robots that move with a significantly higher speed than the TurtleBot used for the simulation.\par
Furthermore, the system was designed and tested with a LiDAR producing relatively sparse point clouds. In the case of usage of some higher quality alternatives, like depth cameras, producing significantly denser point clouds, the system can suffer from performance issues. However, applying any known downsampling technique before starting the fusion can quickly solve this problem.

\section{Future Work}

The neural system of humans or other mammals is still not wholly inspected and is still a prominent research topic. Therefore, there could come many new discoveries that can serve as an inspiration for the improvement of the RatSLAM or any other biologically inspired SLAM system. For example, the spike cells \cite{SpikeNN} represent a more accurate model of mammal neural cells and could be effectively used in the RatSLAM's grid network.\par
Different kinds of neural networks could also be tested for the second stage of the 2-stage approach. For example, HTM \cite{HTM} proved interesting results in combination with RatSLAM. Furthermore, the PointNet network, used as a feature extractor for the siamese network, can be trained on more suitable datasets. Because of the time constraints and limited offer of publicly available datasets, the network in this work was trained on a dataset containing only single objects instead of whole scenes. Even if this approach proved satisfactory, the more suitable training datasets might also improve the final accuracy of the results.\par
There are also additional sensors that could further improve the final results in combination with other sensors, like Dynamic Vision Sensors that can improve scene detection with moving objects. The used camera and LiDAR also have a limited field of view. Unlimited scenes could be detected using a panoramic camera and 360Â° 3D LiDAR. In this case, the scene recognition algorithm could also calculate a relative deviation between two differently rotated locations, which could positively influence the SLAM algorithm. Finally, these additional sensors can also be used to improve the accuracy of the odometry together with the scene recognition.\par
The color model could also be improved. For example, the Lab sphere can be divided into several sectors. The color would be represented as the id of the sector on the sphere instead of the exact Lab value. Instead of applying the CIE formula, the differences between sectors could be tabulated. This could save computational time and some memory and improve the algorithm's robustness against different light conditions.
